"""PDF processing assets for extracting text and generating embeddings."""
import hashlib
import uuid
from datetime import datetime
from pathlib import Path
from typing import List, TypedDict

from dagster import (
    AssetExecutionContext,
    Config,
    StaticPartitionsDefinition,
    asset,
    MaterializeResult,
    MetadataValue,
)
from dagster_weaviate import WeaviateResource
from pypdf import PdfReader
from weaviate.classes.config import Configure, Property, DataType
from weaviate.classes.query import Filter


# Get the project root and features directory
_PROJECT_ROOT = Path(__file__).parent.parent.parent.parent
_FEATURES_DIR = _PROJECT_ROOT / "features"


def _discover_feature_folders() -> List[str]:
    """Discover all folders in the features directory."""
    if not _FEATURES_DIR.exists():
        return []
    
    folders = [
        folder.name
        for folder in _FEATURES_DIR.iterdir()
        if folder.is_dir() and not folder.name.startswith(".")
    ]
    return sorted(folders)


# Create partition definition based on folders in features/
feature_folders = _discover_feature_folders()
folder_partitions_def = StaticPartitionsDefinition(feature_folders) if feature_folders else StaticPartitionsDefinition(["default"])


class PDFProcessingConfig(Config):
    """Configuration for PDF processing.
    
    Note: Embeddings are automatically generated by Weaviate's text2vec-transformers module.
    This config only controls text chunking parameters.
    """

    chunk_size: int = 3000  # Characters per chunk (multi-qa-MiniLM-L6-cos-v1 handles up to ~2000 chars optimally, but larger chunks provide more context)
    chunk_overlap: int = 400  # Overlap between chunks to preserve context (proportional to chunk_size)


class PDFInfo(TypedDict):
    """Type definition for PDF file information."""

    id: str
    filename: str
    folder: str
    pathname: str
    content_type: str


class ExtractedText(TypedDict, total=False):
    """Type definition for extracted PDF text."""

    resource_id: str
    filename: str
    folder: str
    pathname: str
    content_type: str
    text: str
    page_count: int
    error: str


class EmbeddingData(TypedDict):
    """Type definition for embedding data."""

    resource_id: str
    content: str
    embedding: List[float]


# Asset keys for explicit references (using string keys)
DISCOVER_PDFS_KEY = ["pdf_processing", "discover_pdfs"]
EXTRACT_PDF_TEXT_KEY = ["pdf_processing", "extract_pdf_text"]
GENERATE_CHUNKS_KEY = ["pdf_processing", "generate_chunks"]
SAVE_EMBEDDINGS_KEY = ["pdf_processing", "save_embeddings"]


def _chunk_text(text: str, chunk_size: int, chunk_overlap: int) -> List[str]:
    """
    Split text into overlapping chunks.
    
    Args:
        text: The text to chunk
        chunk_size: Maximum characters per chunk
        chunk_overlap: Number of characters to overlap between chunks
    
    Returns:
        List of text chunks
    """
    if len(text) <= chunk_size:
        return [text]
    
    chunks = []
    start = 0
    
    while start < len(text):
        end = start + chunk_size
        chunk = text[start:end]
        chunks.append(chunk)
        
        # Move start position forward, accounting for overlap
        start = end - chunk_overlap
        
        # Prevent infinite loop if overlap is too large
        if chunk_overlap >= chunk_size:
            break
    
    return chunks


@asset(
    key=DISCOVER_PDFS_KEY,
    description="Discover all PDF files in the configured folder partition.",
    group_name="pdf_processing",
    partitions_def=folder_partitions_def,
    kinds={"file"},
)
def discover_pdfs(
    context: AssetExecutionContext,
) -> List[dict]:
    """
    Discover all PDF files in the partition's folder.
    
    Scans the folder corresponding to the current partition recursively for PDF files
    and generates unique IDs based on file paths. Returns list of discovered PDF file information.
    """
    # Get the partition key (folder name)
    try:
        partition_key = context.partition_key
    except Exception:
        available_partitions = folder_partitions_def.get_partition_keys()
        raise ValueError(
            f"This asset requires a partition to be selected. "
            f"Available partitions: {', '.join(available_partitions)}. "
            f"Please select a partition when materializing this asset."
        )
    
    folder_path = _FEATURES_DIR / partition_key
    
    pdf_files: List[PDFInfo] = []
    
    if not folder_path.exists():
        context.log.warning(f"Folder does not exist: {folder_path}")
        context.add_output_metadata(
            {
                "pdfs_discovered": MetadataValue.int(0),
                "folder_path": MetadataValue.path(str(folder_path)),
                "partition": MetadataValue.text(partition_key),
            }
        )
        return pdf_files
    
    context.log.info(f"Scanning folder: {folder_path} (partition: {partition_key})")
    
    for pdf_file in folder_path.rglob("*.pdf"):
        # Generate a unique ID based on file path
        file_id = hashlib.md5(str(pdf_file.absolute()).encode()).hexdigest()
        
        pdf_info: PDFInfo = {
            "id": file_id,
            "filename": pdf_file.name,
            "folder": partition_key,
            "pathname": str(pdf_file.absolute()),
            "content_type": "application/pdf",
        }
        pdf_files.append(pdf_info)
        context.log.info(f"Discovered PDF: {pdf_file.name} ({file_id[:8]}...)")
    
    context.log.info(f"Total PDFs discovered: {len(pdf_files)} in partition '{partition_key}'")
    
    context.add_output_metadata(
        {
            "pdfs_discovered": MetadataValue.int(len(pdf_files)),
            "folder": MetadataValue.text(partition_key),
            "folder_path": MetadataValue.path(str(folder_path)),
            "partition": MetadataValue.text(partition_key),
        }
    )
    
    return pdf_files


@asset(
    key=EXTRACT_PDF_TEXT_KEY,
    description="Extract text content from discovered PDF files using PyPDF.",
    group_name="pdf_processing",
    partitions_def=folder_partitions_def,
    kinds={"pdf"},
    deps=[DISCOVER_PDFS_KEY],
)
def extract_pdf_text(
    context: AssetExecutionContext,
    discover_pdfs: List[dict],
) -> List[dict]:
    """
    Extract text from each discovered PDF file.
    
    Processes each PDF and extracts text content from all pages.
    Handles extraction errors gracefully and tracks success/failure rates.
    """
    extracted_texts: List[ExtractedText] = []
    
    successful_extractions = 0
    failed_extractions = 0
    total_pages = 0
    total_characters = 0
    
    for pdf_info in discover_pdfs:
        pdf_path = Path(pdf_info["pathname"])
        
        try:
            context.log.info(f"Extracting text from: {pdf_info['filename']}")
            
            reader = PdfReader(pdf_path)
            text_content = ""
            
            for page_num, page in enumerate(reader.pages, start=1):
                page_text = page.extract_text()
                text_content += f"\n--- Page {page_num} ---\n{page_text}"
            
            page_count = len(reader.pages)
            char_count = len(text_content)
            
            extracted_text: ExtractedText = {
                "resource_id": pdf_info["id"],
                "filename": pdf_info["filename"],
                "folder": pdf_info["folder"],
                "pathname": pdf_info["pathname"],
                "content_type": pdf_info["content_type"],
                "text": text_content,
                "page_count": page_count,
            }
            extracted_texts.append(extracted_text)
            
            successful_extractions += 1
            total_pages += page_count
            total_characters += char_count
            
            context.log.info(
                f"Extracted {char_count} characters from {page_count} pages "
                f"in {pdf_info['filename']}"
            )
            
        except Exception as e:
            context.log.error(
                f"Error extracting text from {pdf_info['filename']}: {str(e)}"
            )
            failed_extractions += 1
            
            extracted_text: ExtractedText = {
                "resource_id": pdf_info["id"],
                "filename": pdf_info["filename"],
                "folder": pdf_info["folder"],
                "pathname": pdf_info["pathname"],
                "content_type": pdf_info["content_type"],
                "text": "",
                "page_count": 0,
                "error": str(e),
            }
            extracted_texts.append(extracted_text)
    
    # Add metadata about extraction results
    context.add_output_metadata(
        {
            "total_documents": MetadataValue.int(len(extracted_texts)),
            "successful_extractions": MetadataValue.int(successful_extractions),
            "failed_extractions": MetadataValue.int(failed_extractions),
            "total_pages": MetadataValue.int(total_pages),
            "total_characters": MetadataValue.int(total_characters),
            "success_rate": MetadataValue.float(
                float(successful_extractions / len(extracted_texts) * 100) if extracted_texts else 0.0
            ),
        }
    )
    
    return extracted_texts


@asset(
    key=GENERATE_CHUNKS_KEY,
    description="Chunk PDF text for embedding generation. Embeddings will be auto-generated by Weaviate.",
    group_name="pdf_processing",
    partitions_def=folder_partitions_def,
    deps=[EXTRACT_PDF_TEXT_KEY],
    kinds={"pdf"},
)
def generate_chunks(
    context: AssetExecutionContext,
    config: PDFProcessingConfig,
    extract_pdf_text: List[dict],
) -> List[dict]:
    """
    Chunk PDF text for embedding generation.
    
    Chunks text from successful extractions. Embeddings will be automatically
    generated by Weaviate's text2vec-transformers module when saving to Weaviate.
    Large texts are automatically chunked to fit within token limits.
    Skips PDFs that failed text extraction.
    """
    chunks_data: List[dict] = []
    
    successful_chunks = 0
    skipped_chunks = 0
    failed_chunks = 0
    total_chunks = 0
    successful_pdfs = 0
    skipped_pdfs = 0
    failed_pdfs = 0
    
    for pdf_data in extract_pdf_text:
        if not pdf_data.get("text") or pdf_data.get("error"):
            context.log.warning(
                f"Skipping chunking for {pdf_data['filename']} "
                "due to extraction error"
            )
            skipped_pdfs += 1
            continue
        
        try:
            text = pdf_data["text"]
            text_length = len(text)
            
            # Chunk the text if it's too long
            chunks = _chunk_text(text, config.chunk_size, config.chunk_overlap)
            num_chunks = len(chunks)
            
            if num_chunks > 1:
                context.log.info(
                    f"Chunking {pdf_data['filename']} into {num_chunks} chunks "
                    f"(text length: {text_length} chars)"
                )
            
            # Store chunks with metadata (embeddings will be generated by Weaviate)
            for chunk_idx, chunk in enumerate(chunks):
                chunk_data: dict = {
                    "resource_id": pdf_data["resource_id"],
                    "content": chunk,  # Weaviate will auto-generate embedding from this
                    "chunk_index": chunk_idx if num_chunks > 1 else None,
                    "total_chunks": num_chunks if num_chunks > 1 else None,
                    # PDF metadata for Weaviate storage
                    "filename": pdf_data["filename"],
                    "folder": pdf_data["folder"],
                    "pathname": pdf_data["pathname"],
                    "content_type": pdf_data["content_type"],
                }
                chunks_data.append(chunk_data)
                total_chunks += 1
                successful_chunks += 1  # Count each successful chunk
            
            successful_pdfs += 1  # Count successful PDFs
            context.log.info(
                f"Chunked {num_chunks} chunk(s) for {pdf_data['filename']}"
            )
            
        except Exception as e:
            failed_pdfs += 1
            context.log.error(
                f"Error processing {pdf_data['filename']}: {str(e)}"
            )
            # Note: failed_pdfs means no chunks were created for this PDF
    
    # Add metadata about chunking results
    context.add_output_metadata(
        {
            "total_chunks": MetadataValue.int(total_chunks),
            "successful_chunks": MetadataValue.int(successful_chunks),
            "skipped_chunks": MetadataValue.int(skipped_chunks),
            "failed_chunks": MetadataValue.int(failed_chunks),
            "successful_pdfs": MetadataValue.int(successful_pdfs),
            "skipped_pdfs": MetadataValue.int(skipped_pdfs),
            "failed_pdfs": MetadataValue.int(failed_pdfs),
        }
    )

    # If no chunks were created, raise an error
    if len(chunks_data) == 0:
        error_msg = "No chunks were created. All attempts to chunk text failed."
        context.log.error(error_msg)
        raise ValueError(error_msg)
    
    return chunks_data


@asset(
    key=SAVE_EMBEDDINGS_KEY,
    description="Save text chunks to Weaviate. Embeddings are auto-generated by Weaviate's text2vec-transformers module.",
    group_name="pdf_processing",
    partitions_def=folder_partitions_def,
    deps=[GENERATE_CHUNKS_KEY],
    kinds={"weaviate"},
)
def save_embeddings(
    context: AssetExecutionContext,
    weaviate: WeaviateResource,
    generate_chunks: List[dict],
) -> MaterializeResult:
    """
    Save text chunks to Weaviate with metadata. Embeddings are auto-generated.
    
    Stores text chunks in Weaviate with all PDF metadata (filename, folder, 
    pathname, content_type, chunk_index, total_chunks). Weaviate automatically
    generates embeddings from the 'content' field using text2vec-transformers.
    Deletes existing chunks for each resource_id before inserting new ones
    to ensure idempotency and prevent duplicates on re-runs.
    """
    
    saved_count = 0
    error_count = 0
    deleted_resources = 0
    
    try:
        with weaviate.get_client() as weaviate_client:
            # Collection name (Weaviate v4 uses lowercase)
            collection_name = "embeddings"
            
            # Check if collection exists and recreate if needed for auto-vectorization
            try:
                collection = weaviate_client.collections.get(collection_name)
                if collection.exists():
                    # Check if it has vectorization configured
                    # If not, we'll need to delete and recreate (handled below)
                    context.log.info(f"Collection '{collection_name}' exists")
            except Exception:
                collection = None
            
            # Create collection with text2vec-transformers for auto-embedding
            # If collection exists without vectorization, delete it first
            if collection is None or not collection.exists():
                try:
                    # Try to delete old collection if it exists (might be from v1 API)
                    weaviate_client.collections.delete(collection_name)
                    context.log.info(f"Deleted old collection '{collection_name}' to recreate with vectorization")
                except Exception:
                    pass  # Collection doesn't exist, that's fine
                
                # Create the collection with text2vec-transformers for auto-embedding
                collection = weaviate_client.collections.create(
                    name=collection_name,
                    vector_config=Configure.Vectors.text2vec_transformers(
                        vectorize_collection_name=False
                    ),
                    properties=[
                        Property(name="content", data_type=DataType.TEXT),
                        Property(name="resource_id", data_type=DataType.TEXT),
                        Property(name="filename", data_type=DataType.TEXT),
                        Property(name="folder", data_type=DataType.TEXT),
                        Property(name="pathname", data_type=DataType.TEXT),
                        Property(name="content_type", data_type=DataType.TEXT),
                        Property(name="chunk_index", data_type=DataType.INT),
                        Property(name="total_chunks", data_type=DataType.INT),
                    ],
                )
                context.log.info(f"Created collection '{collection_name}' with auto-vectorization")

            # Group chunks by resource_id to delete all chunks for a resource at once
            resource_ids = set(chunk_data["resource_id"] for chunk_data in generate_chunks)
            
            # Delete existing chunks for all resources being processed
            for resource_id in resource_ids:
                try:
                    collection.data.delete_many(where=Filter.by_property("resource_id").equal(resource_id))
                    deleted_resources += 1
                    context.log.debug(
                        f"Deleted existing chunks for resource: {resource_id[:8]}..."
                    )
                except Exception as e:
                    context.log.warning(
                        f"Error deleting existing chunks for resource {resource_id[:8]}...: {str(e)}"
                    )

            # Prepare data for batch insertion
            # Weaviate will automatically generate embeddings from the 'content' field
            properties_list = []
            ids_list = []
            
            for chunk_data in generate_chunks:
                try:
                    embedding_id = str(uuid.uuid4())
                    
                    # Get chunk_index (defaults to 0 if not set, for single-chunk PDFs)
                    chunk_index = chunk_data.get("chunk_index")
                    if chunk_index is None:
                        chunk_index = 0
                    
                    # Prepare properties for Weaviate
                    # The 'content' field will be automatically vectorized by Weaviate's text2vec-transformers
                    properties = {
                        "content": chunk_data["content"],  # Auto-vectorized by Weaviate
                        "resource_id": chunk_data["resource_id"],
                        "filename": chunk_data["filename"],
                        "folder": chunk_data["folder"],
                        "pathname": chunk_data["pathname"],
                        "content_type": chunk_data["content_type"],
                        "chunk_index": chunk_index,
                    }
                    
                    # Add total_chunks if it exists
                    total_chunks = chunk_data.get("total_chunks")
                    if total_chunks is not None:
                        properties["total_chunks"] = total_chunks
                    
                    properties_list.append(properties)
                    ids_list.append(embedding_id)
                    
                except Exception as e:
                    error_count += 1
                    context.log.error(
                        f"Error preparing chunk for resource "
                        f"{chunk_data.get('resource_id', 'unknown')}: {str(e)}"
                    )
            
            # Batch insert all chunks (embeddings auto-generated by Weaviate)
            if ids_list:
                try:
                    # Use Weaviate's batch API for efficient bulk insertion
                    # Weaviate automatically generates embeddings from the 'content' field via text2vec-transformers
                    with collection.batch.fixed_size(batch_size=100) as batch:
                        for i in range(len(ids_list)):
                            batch.add_object(
                                properties=properties_list[i],
                                uuid=ids_list[i]
                            )

                    # All chunks in ids_list were successfully added to batch
                    # Note: actual insertion happens when batch context exits
                    saved_count = len(ids_list)
                    context.log.info(f"Successfully saved {saved_count} chunk(s) to Weaviate (embeddings auto-generated)")
                except Exception as e:
                    # If batch insertion fails, count all as errors
                    # Note: Weaviate batch API doesn't provide partial success info
                    error_count += len(ids_list) - saved_count
                    context.log.error(f"Error batch inserting chunks to Weaviate: {str(e)}")
                    raise

    except Exception as e:
        context.log.error(f"Error saving chunks to Weaviate: {str(e)}")
        raise
    
    # If all chunks failed to save, raise an error
    if error_count > 0 and saved_count == 0:
        error_msg = (
            f"Failed to save all {len(generate_chunks)} chunks. "
            f"All {error_count} attempts resulted in errors."
        )
        context.log.error(error_msg)
        raise ValueError(error_msg)
    
    # If some chunks failed, log a warning but don't fail
    if error_count > 0:
        context.log.warning(
            f"Failed to save {error_count} out of {len(generate_chunks)} chunks. "
            f"Successfully saved {saved_count}."
        )
    
    return MaterializeResult(
        metadata={
            "saved": MetadataValue.int(saved_count),
            "deleted_resources": MetadataValue.int(deleted_resources),
            "errors": MetadataValue.int(error_count),
            "total": MetadataValue.int(len(generate_chunks)),
            "timestamp": MetadataValue.timestamp(datetime.now().timestamp()),
        }
    )
